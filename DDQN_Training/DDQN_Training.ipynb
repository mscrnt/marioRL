{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu117\n",
      "Requirement already satisfied: torch in c:\\users\\kenneth\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (1.10.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\kenneth\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch) (4.4.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu117\n",
      "Requirement already satisfied: torchvision in c:\\users\\kenneth\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.11.2+cu113)\n",
      "Requirement already satisfied: numpy in c:\\users\\kenneth\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchvision) (1.23.4)\n",
      "Requirement already satisfied: torch==1.10.1 in c:\\users\\kenneth\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchvision) (1.10.1)\n",
      "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in c:\\users\\kenneth\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchvision) (9.3.0)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\kenneth\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==1.10.1->torchvision) (4.4.0)\n",
      "Looking in indexes: https://pypi.org/simple, https://download.pytorch.org/whl/cu117\n",
      "Requirement already satisfied: torchaudio in c:\\users\\kenneth\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (0.10.1+cu113)\n",
      "Requirement already satisfied: torch==1.10.1 in c:\\users\\kenneth\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torchaudio) (1.10.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\kenneth\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from torch==1.10.1->torchaudio) (4.4.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch --extra-index-url https://download.pytorch.org/whl/cu117\n",
    "!pip install torchvision --extra-index-url https://download.pytorch.org/whl/cu117\n",
    "!pip install torchaudio --extra-index-url https://download.pytorch.org/whl/cu117"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import PIL.ImageDraw as ImageDraw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kenneth\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "  0%|          | 0/100 [00:00<?, ?it/s]C:\\Users\\Kenneth\\AppData\\Local\\Temp\\ipykernel_20216\\1934949962.py:359: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at  ..\\torch\\csrc\\utils\\tensor_new.cpp:201.)\n",
      "  state = torch.Tensor([state])\n",
      "c:\\Users\\Kenneth\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\pyglet\\image\\codecs\\wic.py:289: UserWarning: [WinError -2147417850] Cannot change thread mode after it is set\n",
      "  warnings.warn(str(err))\n",
      "  5%|â–Œ         | 5/100 [00:22<07:02,  4.45s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [2], line 412\u001b[0m\n\u001b[0;32m    409\u001b[0m     env\u001b[39m.\u001b[39mclose()\n\u001b[0;32m    411\u001b[0m \u001b[39m# For training\u001b[39;00m\n\u001b[1;32m--> 412\u001b[0m run(training_mode\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, pretrained\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m, double_dqn\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m, num_episodes\u001b[39m=\u001b[39;49m\u001b[39m100\u001b[39;49m, exploration_max \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m)\n\u001b[0;32m    414\u001b[0m \u001b[39m# For Testing\u001b[39;00m\n\u001b[0;32m    415\u001b[0m \u001b[39m#run(training_mode=False, pretrained=True, double_dqn=True, num_episodes=100, exploration_max = 0.05)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn [2], line 376\u001b[0m, in \u001b[0;36mrun\u001b[1;34m(training_mode, pretrained, double_dqn, num_episodes, exploration_max)\u001b[0m\n\u001b[0;32m    373\u001b[0m terminal \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mtensor([\u001b[39mint\u001b[39m(terminal)])\u001b[39m.\u001b[39munsqueeze(\u001b[39m0\u001b[39m)\n\u001b[0;32m    375\u001b[0m \u001b[39mif\u001b[39;00m training_mode:\n\u001b[1;32m--> 376\u001b[0m     agent\u001b[39m.\u001b[39;49mremember(state, action, reward, state_next, terminal)\n\u001b[0;32m    377\u001b[0m     agent\u001b[39m.\u001b[39mexperience_replay()\n\u001b[0;32m    379\u001b[0m state \u001b[39m=\u001b[39m state_next\n",
      "Cell \u001b[1;32mIn [2], line 223\u001b[0m, in \u001b[0;36mDQNAgent.remember\u001b[1;34m(self, state, action, reward, state2, done)\u001b[0m\n\u001b[0;32m    221\u001b[0m \u001b[39m\"\"\"Store the experiences in a buffer to use later\"\"\"\u001b[39;00m\n\u001b[0;32m    222\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mSTATE_MEM[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mending_position] \u001b[39m=\u001b[39m state\u001b[39m.\u001b[39mfloat()\n\u001b[1;32m--> 223\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mACTION_MEM[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mending_position] \u001b[39m=\u001b[39m action\u001b[39m.\u001b[39;49mfloat()\n\u001b[0;32m    224\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mREWARD_MEM[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mending_position] \u001b[39m=\u001b[39m reward\u001b[39m.\u001b[39mfloat()\n\u001b[0;32m    225\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mSTATE2_MEM[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mending_position] \u001b[39m=\u001b[39m state2\u001b[39m.\u001b[39mfloat()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import random\n",
    "from nes_py.wrappers import JoypadSpace\n",
    "import gym_super_mario_bros\n",
    "from tqdm import tqdm\n",
    "import pickle \n",
    "from gym_super_mario_bros.actions import RIGHT_ONLY, SIMPLE_MOVEMENT, COMPLEX_MOVEMENT\n",
    "import gym\n",
    "import numpy as np\n",
    "import collections \n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "import time\n",
    "import pylab as pl\n",
    "from IPython import display\n",
    "\n",
    "class MaxAndSkipEnv(gym.Wrapper):\n",
    "    \"\"\"\n",
    "        Each action of the agent is repeated over skip frames\n",
    "        return only every `skip`-th frame\n",
    "    \"\"\"\n",
    "    def __init__(self, env=None, skip=4):\n",
    "        super(MaxAndSkipEnv, self).__init__(env)\n",
    "        # most recent raw observations (for max pooling across time steps)\n",
    "        self._obs_buffer = collections.deque(maxlen=2)\n",
    "        self._skip = skip\n",
    "\n",
    "    def step(self, action):\n",
    "        total_reward = 0.0\n",
    "        done = None\n",
    "        for _ in range(self._skip):\n",
    "            obs, reward, done, info = self.env.step(action)\n",
    "            self._obs_buffer.append(obs)\n",
    "            total_reward += reward\n",
    "            if done:\n",
    "                break\n",
    "        max_frame = np.max(np.stack(self._obs_buffer), axis=0)\n",
    "        return max_frame, total_reward, done, info\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Clear past frame buffer and init to first obs\"\"\"\n",
    "        self._obs_buffer.clear()\n",
    "        obs = self.env.reset()\n",
    "        self._obs_buffer.append(obs)\n",
    "        return obs\n",
    "\n",
    "\n",
    "class MarioRescale84x84(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Downsamples/Rescales each frame to size 84x84 with greyscale\n",
    "    \"\"\"\n",
    "    def __init__(self, env=None):\n",
    "        super(MarioRescale84x84, self).__init__(env)\n",
    "        self.observation_space = gym.spaces.Box(low=0, high=255, shape=(84, 84, 1), dtype=np.uint8)\n",
    "\n",
    "    def observation(self, obs):\n",
    "        return MarioRescale84x84.process(obs)\n",
    "\n",
    "    @staticmethod\n",
    "    def process(frame):\n",
    "        if frame.size == 240 * 256 * 3:\n",
    "            img = np.reshape(frame, [240, 256, 3]).astype(np.float32)\n",
    "        else:\n",
    "            assert False, \"Unknown resolution.\" \n",
    "        # image normalization on RBG\n",
    "        img = img[:, :, 0] * 0.299 + img[:, :, 1] * 0.587 + img[:, :, 2] * 0.114\n",
    "        resized_screen = cv2.resize(img, (84, 110), interpolation=cv2.INTER_AREA)\n",
    "        x_t = resized_screen[18:102, :]\n",
    "        x_t = np.reshape(x_t, [84, 84, 1])\n",
    "        return x_t.astype(np.uint8)\n",
    "\n",
    "\n",
    "class ImageToPyTorch(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Each frame is converted to PyTorch tensors\n",
    "    \"\"\"\n",
    "    def __init__(self, env):\n",
    "        super(ImageToPyTorch, self).__init__(env)\n",
    "        old_shape = self.observation_space.shape\n",
    "        self.observation_space = gym.spaces.Box(low=0.0, high=1.0, shape=(old_shape[-1], old_shape[0], old_shape[1]), dtype=np.float32)\n",
    "\n",
    "    def observation(self, observation):\n",
    "        return np.moveaxis(observation, 2, 0)\n",
    "\n",
    "    \n",
    "class BufferWrapper(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Only every k-th frame is collected by the buffer\n",
    "    \"\"\"\n",
    "    def __init__(self, env, n_steps, dtype=np.float32):\n",
    "        super(BufferWrapper, self).__init__(env)\n",
    "        self.dtype = dtype\n",
    "        old_space = env.observation_space\n",
    "        self.observation_space = gym.spaces.Box(old_space.low.repeat(n_steps, axis=0),\n",
    "                                                old_space.high.repeat(n_steps, axis=0), dtype=dtype)\n",
    "\n",
    "    def reset(self):\n",
    "        self.buffer = np.zeros_like(self.observation_space.low, dtype=self.dtype)\n",
    "        return self.observation(self.env.reset())\n",
    "\n",
    "    def observation(self, observation):\n",
    "        self.buffer[:-1] = self.buffer[1:]\n",
    "        self.buffer[-1] = observation\n",
    "        return self.buffer\n",
    "\n",
    "\n",
    "class PixelNormalization(gym.ObservationWrapper):\n",
    "    \"\"\"\n",
    "    Normalize pixel values in frame --> 0 to 1\n",
    "    \"\"\"\n",
    "    def observation(self, obs):\n",
    "        return np.array(obs).astype(np.float32) / 255.0\n",
    "\n",
    "\n",
    "def create_mario_env(env):\n",
    "    env = MaxAndSkipEnv(env)\n",
    "    env = MarioRescale84x84(env)\n",
    "    env = ImageToPyTorch(env)\n",
    "    env = BufferWrapper(env, 4)\n",
    "    env = PixelNormalization(env)\n",
    "    return JoypadSpace(env, SIMPLE_MOVEMENT)\n",
    "  \n",
    "class DQNSolver(nn.Module):\n",
    "    \"\"\"\n",
    "    Convolutional Neural Net with 3 conv layers and two linear layers\n",
    "    \"\"\"\n",
    "    def __init__(self, input_shape, n_actions):\n",
    "        super(DQNSolver, self).__init__()\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(input_shape[0], 32, kernel_size=8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, stride=1),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        conv_out_size = self._get_conv_out(input_shape)\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(conv_out_size, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, n_actions)\n",
    "        )\n",
    "    \n",
    "    def _get_conv_out(self, shape):\n",
    "        o = self.conv(torch.zeros(1, *shape))\n",
    "        return int(np.prod(o.size()))\n",
    "\n",
    "    def forward(self, x):\n",
    "        conv_out = self.conv(x).view(x.size()[0], -1)\n",
    "        return self.fc(conv_out)\n",
    "\n",
    "\n",
    "class DQNAgent:\n",
    "\n",
    "    def __init__(self, state_space, action_space, max_memory_size, batch_size, gamma, lr,\n",
    "                 dropout, exploration_max, exploration_min, exploration_decay, double_dqn, pretrained):\n",
    "\n",
    "        # Define DQN Layers\n",
    "        self.state_space = state_space\n",
    "        self.action_space = action_space\n",
    "        self.double_dqn = double_dqn\n",
    "        self.pretrained = pretrained\n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "        \n",
    "        # Double DQN network\n",
    "        if self.double_dqn:  \n",
    "            self.local_net = DQNSolver(state_space, action_space).to(self.device)\n",
    "            self.target_net = DQNSolver(state_space, action_space).to(self.device)\n",
    "            \n",
    "            if self.pretrained:\n",
    "                self.local_net.load_state_dict(torch.load(\"DQN1.pt\", map_location=torch.device(self.device)))\n",
    "                self.target_net.load_state_dict(torch.load(\"DQN2.pt\", map_location=torch.device(self.device)))\n",
    "                    \n",
    "            self.optimizer = torch.optim.Adam(self.local_net.parameters(), lr=lr)\n",
    "            self.copy = 5000  # Copy the local model weights into the target network every 5000 steps\n",
    "            self.step = 0\n",
    "        # DQN network\n",
    "        else:  \n",
    "            self.dqn = DQNSolver(state_space, action_space).to(self.device)\n",
    "            \n",
    "            if self.pretrained:\n",
    "                self.dqn.load_state_dict(torch.load(\"DQN.pt\", map_location=torch.device(self.device)))\n",
    "            self.optimizer = torch.optim.Adam(self.dqn.parameters(), lr=lr)\n",
    "\n",
    "        # Create memory\n",
    "        self.max_memory_size = max_memory_size\n",
    "        if self.pretrained:\n",
    "            self.STATE_MEM = torch.load(\"STATE_MEM.pt\")\n",
    "            self.ACTION_MEM = torch.load(\"ACTION_MEM.pt\")\n",
    "            self.REWARD_MEM = torch.load(\"REWARD_MEM.pt\")\n",
    "            self.STATE2_MEM = torch.load(\"STATE2_MEM.pt\")\n",
    "            self.DONE_MEM = torch.load(\"DONE_MEM.pt\")\n",
    "            with open(\"ending_position.pkl\", 'rb') as f:\n",
    "                self.ending_position = pickle.load(f)\n",
    "            with open(\"num_in_queue.pkl\", 'rb') as f:\n",
    "                self.num_in_queue = pickle.load(f)\n",
    "        else:\n",
    "            self.STATE_MEM = torch.zeros(max_memory_size, *self.state_space)\n",
    "            self.ACTION_MEM = torch.zeros(max_memory_size, 1)\n",
    "            self.REWARD_MEM = torch.zeros(max_memory_size, 1)\n",
    "            self.STATE2_MEM = torch.zeros(max_memory_size, *self.state_space)\n",
    "            self.DONE_MEM = torch.zeros(max_memory_size, 1)\n",
    "            self.ending_position = 0\n",
    "            self.num_in_queue = 0\n",
    "        \n",
    "        self.memory_sample_size = batch_size\n",
    "        \n",
    "        # Learning parameters\n",
    "        self.gamma = gamma\n",
    "        self.l1 = nn.SmoothL1Loss().to(self.device) # Also known as Huber loss\n",
    "        self.exploration_max = exploration_max\n",
    "        self.exploration_rate = exploration_max\n",
    "        self.exploration_min = exploration_min\n",
    "        self.exploration_decay = exploration_decay\n",
    "\n",
    "    def remember(self, state, action, reward, state2, done):\n",
    "        \"\"\"Store the experiences in a buffer to use later\"\"\"\n",
    "        self.STATE_MEM[self.ending_position] = state.float()\n",
    "        self.ACTION_MEM[self.ending_position] = action.float()\n",
    "        self.REWARD_MEM[self.ending_position] = reward.float()\n",
    "        self.STATE2_MEM[self.ending_position] = state2.float()\n",
    "        self.DONE_MEM[self.ending_position] = done.float()\n",
    "        self.ending_position = (self.ending_position + 1) % self.max_memory_size  # FIFO tensor\n",
    "        self.num_in_queue = min(self.num_in_queue + 1, self.max_memory_size)\n",
    "    \n",
    "    def batch_experiences(self):\n",
    "        \"\"\"Randomly sample 'batch size' experiences\"\"\"\n",
    "        idx = random.choices(range(self.num_in_queue), k=self.memory_sample_size)\n",
    "        STATE = self.STATE_MEM[idx]\n",
    "        ACTION = self.ACTION_MEM[idx]\n",
    "        REWARD = self.REWARD_MEM[idx]\n",
    "        STATE2 = self.STATE2_MEM[idx]\n",
    "        DONE = self.DONE_MEM[idx]      \n",
    "        return STATE, ACTION, REWARD, STATE2, DONE\n",
    "    \n",
    "    def act(self, state):\n",
    "        \"\"\"Epsilon-greedy action\"\"\"\n",
    "        if self.double_dqn:\n",
    "            self.step += 1\n",
    "        if random.random() < self.exploration_rate:  \n",
    "            return torch.tensor([[random.randrange(self.action_space)]])\n",
    "        if self.double_dqn:\n",
    "            # Local net is used for the policy\n",
    "            return torch.argmax(self.local_net(state.to(self.device))).unsqueeze(0).unsqueeze(0).cpu()\n",
    "        else:\n",
    "            return torch.argmax(self.dqn(state.to(self.device))).unsqueeze(0).unsqueeze(0).cpu()\n",
    "    \n",
    "    def copy_model(self):\n",
    "        \"\"\"Copy local net weights into target net for DDQN network\"\"\"\n",
    "        self.target_net.load_state_dict(self.local_net.state_dict())\n",
    "    \n",
    "    def experience_replay(self):\n",
    "        \"\"\"Use the double Q-update or Q-update equations to update the network weights\"\"\"\n",
    "        if self.double_dqn and self.step % self.copy == 0:\n",
    "            self.copy_model()\n",
    "\n",
    "        if self.memory_sample_size > self.num_in_queue:\n",
    "            return\n",
    "    \n",
    "        # Sample a batch of experiences\n",
    "        STATE, ACTION, REWARD, STATE2, DONE = self.batch_experiences()\n",
    "        STATE = STATE.to(self.device)\n",
    "        ACTION = ACTION.to(self.device)\n",
    "        REWARD = REWARD.to(self.device)\n",
    "        STATE2 = STATE2.to(self.device)\n",
    "        DONE = DONE.to(self.device)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        if self.double_dqn:\n",
    "            # Double Q-Learning target is Q*(S, A) <- r + Î³ max_a Q_target(S', a)\n",
    "            target = REWARD + torch.mul((self.gamma * self.target_net(STATE2).max(1).values.unsqueeze(1)),  1 - DONE)\n",
    "\n",
    "            current = self.local_net(STATE).gather(1, ACTION.long()) # Local net approximation of Q-value\n",
    "        else:\n",
    "            # Q-Learning target is Q*(S, A) <- r + Î³ max_a Q(S', a) \n",
    "            target = REWARD + torch.mul((self.gamma * self.dqn(STATE2).max(1).values.unsqueeze(1)), 1 - DONE)\n",
    "                \n",
    "            current = self.dqn(STATE).gather(1, ACTION.long())\n",
    "        \n",
    "        loss = self.l1(current, target)\n",
    "        loss.backward() # Compute gradients\n",
    "        self.optimizer.step() # Backpropagate error\n",
    "\n",
    "        self.exploration_rate *= self.exploration_decay\n",
    "        \n",
    "        # Makes sure that exploration rate is always at least 'exploration min'\n",
    "        self.exploration_rate = max(self.exploration_rate, self.exploration_min)\n",
    "        \n",
    "def show_state(env, ep_num, info=\"\"):\n",
    "    env.render()\n",
    "    \n",
    "\n",
    "\n",
    "def run(training_mode, pretrained, double_dqn, num_episodes=1000, exploration_max=1):\n",
    "   \n",
    "    env = gym_super_mario_bros.make('SuperMarioBros-1-1-v0')\n",
    "    env = create_mario_env(env)  # Wraps the environment so that frames are grayscale \n",
    "    observation_space = env.observation_space.shape\n",
    "    action_space = env.action_space.n\n",
    "    agent = DQNAgent(state_space=observation_space,\n",
    "                     action_space=action_space,\n",
    "                     max_memory_size=30000,\n",
    "                     batch_size=32,\n",
    "                     gamma=0.90,\n",
    "                     lr=0.00025,\n",
    "                     dropout=0.2,\n",
    "                     exploration_max=1.0,\n",
    "                     exploration_min=0.02,\n",
    "                     exploration_decay=0.99,\n",
    "                     double_dqn=double_dqn,\n",
    "                     pretrained=pretrained)\n",
    "    \n",
    "    # Restart the enviroment for each episode\n",
    "    num_episodes = num_episodes\n",
    "    env.reset()\n",
    "    \n",
    "    total_rewards = []\n",
    "    if training_mode and pretrained:\n",
    "        with open(\"total_rewards.pkl\", 'rb') as f:\n",
    "            total_rewards = pickle.load(f)\n",
    "    \n",
    "    for ep_num in tqdm(range(num_episodes)):\n",
    "        state = env.reset()\n",
    "        state = torch.Tensor([state])\n",
    "        total_reward = 0\n",
    "        steps = 0\n",
    "        while True:\n",
    "            if training_mode:\n",
    "                show_state(env, ep_num)\n",
    "            action = agent.act(state)\n",
    "            steps += 1\n",
    "            \n",
    "            state_next, reward, terminal, info = env.step(int(action[0]))\n",
    "            total_reward += reward\n",
    "            state_next = torch.Tensor([state_next])\n",
    "            reward = torch.tensor([reward]).unsqueeze(0)\n",
    "            \n",
    "            terminal = torch.tensor([int(terminal)]).unsqueeze(0)\n",
    "            \n",
    "            if training_mode:\n",
    "                agent.remember(state, action, reward, state_next, terminal)\n",
    "                agent.experience_replay()\n",
    "            \n",
    "            state = state_next\n",
    "            if terminal:\n",
    "                break\n",
    "        \n",
    "        total_rewards.append(total_reward)\n",
    "        \n",
    "        if ep_num != 0 and ep_num % 100 == 0:\n",
    "            print(\"Episode {} score = {}, average score = {}\".format(ep_num + 1, total_rewards[-1], np.mean(total_rewards)))\n",
    "        num_episodes += 1  \n",
    "\n",
    "    print(\"Episode {} score = {}, average score = {}\".format(ep_num + 1, total_rewards[-1], np.mean(total_rewards)))\n",
    "    \n",
    "    # Save the trained memory so that we can continue from where we stop using 'pretrained' = True\n",
    "    if training_mode:\n",
    "        with open(\"ending_position.pkl\", \"wb\") as f:\n",
    "            pickle.dump(agent.ending_position, f)\n",
    "        with open(\"num_in_queue.pkl\", \"wb\") as f:\n",
    "            pickle.dump(agent.num_in_queue, f)\n",
    "        with open(\"total_rewards.pkl\", \"wb\") as f:\n",
    "            pickle.dump(total_rewards, f)\n",
    "        if agent.double_dqn:\n",
    "            torch.save(agent.local_net.state_dict(), \"DQN1.pt\")\n",
    "            torch.save(agent.target_net.state_dict(), \"DQN2.pt\")\n",
    "        else:\n",
    "            torch.save(agent.dqn.state_dict(), \"DQN.pt\")  \n",
    "        torch.save(agent.STATE_MEM,  \"STATE_MEM.pt\")\n",
    "        torch.save(agent.ACTION_MEM, \"ACTION_MEM.pt\")\n",
    "        torch.save(agent.REWARD_MEM, \"REWARD_MEM.pt\")\n",
    "        torch.save(agent.STATE2_MEM, \"STATE2_MEM.pt\")\n",
    "        torch.save(agent.DONE_MEM,   \"DONE_MEM.pt\")\n",
    "    env.close()\n",
    "\n",
    "# For training\n",
    "run(training_mode=True, pretrained=False, double_dqn=True, num_episodes=100, exploration_max = 1)\n",
    "\n",
    "# For Testing\n",
    "#run(training_mode=False, pretrained=True, double_dqn=True, num_episodes=100, exploration_max = 0.05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31b8910ab33c7afb18a70a6c11b82b733b296c5c529ccd380964595a3a81d01b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
